---
title: "intro to \\texttt{bamoso}"
author: "Christof Neumann"
date: "`r Sys.Date()` (built with v. `r packageVersion('bamoso')`)"
output: 
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    number_sections: false
vignette: >
  %\VignetteIndexEntry{intro to bamoso}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
link-citations: yes
linkcolor: red
geometry: margin = 0.7in
bibliography: ../inst/REFERENCES.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# csl: animal-behaviour.csl
knitr::opts_chunk$set(echo = TRUE)
options(width = 120)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(size = 'footnotesize', fig.align = 'center')
# compile time
compile_start <- Sys.time()

```

\vspace{2ex}

This document (and package) is work in progress.
Installation instructions can be found at https://github.com/gobbios/bamoso/.
If you find mistakes or have suggestions, I'd be very happy to hear about them at https://github.com/gobbios/bamoso/issues.

```{r setup, include=FALSE}
library(bamoso)
```

# Terminology

When **interactions** are mentioned in this document we refer to frequencies and durations of dyadic interactions.

**Components** and **axes** are the overarching terms to refer to **affinity** and **gregariousness**. 
E.g. 'Affinity is the dyadic component.'




\clearpage

# Quick start

This first example illustrates how to apply the model to pretty much the simplest possible data set.
We will skip over a lot of the details, and just show the general workflow.

## Preparing the data

Here we use grooming data from 19 Barbary macaques (*Macaca sylvanus*) recorded with focal animal sampling.
These grooming data represent frequencies of grooming bouts (so we need a count model eventually) and are undirected.
We also have information about observation effort, i.e. the time any two individuals *could* have interacted.
The data are included in the `bamoso` package and are illustrated in figure \ref{fig:syl_net} (only six individuals shown there).
The data are stored initially in matrix form.

```{r, echo=FALSE, fig.width=8, fig.height=2.5, out.width="80%", fig.align='center', fig.cap="\\label{fig:syl_net}Example data set. Grooming frequencies on the left and dyadic observation hours on the right."}
par(mar = c(0, 2, 2, 0), family = "serif", cex = 0.7, mfrow = c(1, 2))
data(grooming)
sel <- c(1, 4, 6, 10, 12, 19)
EloSteepness:::plot_matrix(grooming$syl$groom[sel, sel])
EloSteepness:::plot_matrix(grooming$syl$obseff[sel, sel])
```


```{r, eval = FALSE}
library(bamoso)
```

```{r}
data("grooming")
groom_mat <- grooming$syl$groom
obseff_mat <- grooming$syl$obseff
```

Now we need to convert this data set into a format that is suitable to be passed to Stan.
There would be a few things to note here but we postpone this to the more extensive example below.
Here there are no problems with the data and the conversion is straightforward.
We pass a named list with the interaction matrix (`mats = list(groom = groom_mat)`).
We indicate that the data represent frequencies or counts (`behav_types = "count"`).
And we pass the observation effort as list as well (`obseff = list(obseff_mat)`).

```{r}
standat <- make_stan_data_from_matrices(mats = list(groom = groom_mat),
                                        behav_types = "count",
                                        obseff = list(obseff_mat))
```

## Fitting the model    

Now we can fit the model, using the `sociality_model()` function.
This function requires only the converted data set as argument.
If you run this, you might get a warning about a few divergent transitions.
This is a mild problem here, and we postpone showing how to fix this to further below in the more extensive simulated example.

```{r quick_start_model, cache=TRUE, include=FALSE}
res <- sociality_model(standat, parallel_chains = 4, 
                       adapt_delta = 0.9, seed = 2)
```

```{r, eval = FALSE}
res <- sociality_model(standat)
```

Once the model is fitted, we can obtain a quick summary.
If there were sampling issues (like divergent transitions) the summary will report this as well.

```{r}
summary(res)
```

In this document, sampling went without troubles.

Now we can look into model performance by doing posterior predictive checks.
This just means that we look at how well (or badly) the model is able to predict interaction patterns that resemble what we actually observed.
And by 'interaction pattern' we simply mean the distribution of grooming bouts across all dyads here.
There are different ways of going about that, but this one is easy and if there are severe problems, we will be able to detect them here.

```{r, echo=2:5, fig.width=8, fig.height=2.5, out.width="80%", fig.align='center', fig.cap="\\label{fig:syl_net_pp}Posterior predictive checks for Barbary macaque grooming data. On the left: bars represent observed grooming frequencies. The circles represent the median frequencies predicted by the model and the vertical bars the 89\\% interval around these predictions. One the right: the histogram is the posterior distribution of the maximum number of grooming bouts predicted by the model across all posterior samples. The red line indicates the observed value in the data."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5), mfrow = c(1, 2))
pp_model(res, xlab = "grooming bouts", ylab = "(predicted) frequency",
         xbreaks = 20)
pp_model_stat(res, stat = "max", xlab = "maximum number of grooming bouts",
              main = "")
```

The two example checks in figure \ref{fig:syl_net_pp} indicate no obvious problems.

## Visualising the results

Now we can look at the actual results graphically.
We start by looking at the posteriors for the individual and the dyadic component.

```{r, echo=2:5, fig.width=5, fig.height=2.1, out.width="50%", fig.align='center', fig.cap="\\label{fig:syl_net_soc}Posteriors for the two sociality components. Variation among dyads is larger than variation among individuals."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
sociality_plot(res)
```

We can also display the individual-level gregariousness posteriors using `ridge_plot()` (see figure \ref{fig:syl_net_ridge}).
In principle, this is also possible for the dyadic affinity values, but this becomes cluttered quickly if there are too many dyads to display.
In the example in figure \ref{fig:syl_net_ridge} only a subset of dyads is therefore displayed.

```{r, echo=2:5, fig.width=8, fig.height=2.9, out.width="80%", fig.align='center', fig.cap="\\label{fig:syl_net_ridge}Posteriors for the two sociality components. The affinity distributions on the right only reflect a subset of 10 dyads."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5), mfrow = c(1, 2))
ridge_plot(res, greg = TRUE)
ridge_plot(res, greg = FALSE, sel_subset = 10:20, vert_exp = 15)
```

## Numerical results

To extract numerical results, there are two functions: `model_summary()` and `extract_samples()`.
`model_summary()` provides a summary of the posteriors for all the parameters of interest and is simply a data frame.

```{r, results='hide'}
xsum <- model_summary(res)
head(xsum)
```

```{r, echo=FALSE}
print(head(xsum), digits = 3)
```


We only see the first six rows, but the structure hopefully becomes clear.
Each row represents one parameter.
The two top rows represent the estimated variation for the gregariousness and dyadic affinity components (in fact we estimated *standard deviation*).
This is followed by the estimates for the behavior intercept(s) (there is only one here because we only modeled one behavior).
Next are the individual gregariousness values, followed by the estimates for all the dyads.
Alongside the point estimates for the posteriors (mean and median), we get measures of uncertainty around these point estimates (SD, quantiles).
And finally, there are some diagnostic values from Stan (rhat, ess_bulk and ess_tail).

In order to get the full posteriors, we use `extract_samples()`.
This requires specifying the parameter(s) for which we want to return the posteriors.
For example, getting the individual gregariousness values requires `what = "indi_vals"`.\footnote{see the help file (\texttt{?extract\_samples}) for more information.}

```{r}
e <- extract_samples(res, what = "indi_vals")
head(round(e[, 1:5], 3))
```

## Summary

This was a brief run through a simple use case of estimating individual-level and dyad-level propensities to interact.
It illustrated the key functions of the `bamoso` package, mostly with their default settings.

\clearpage

# An example with simulated data

We now turn to an example that uses simulated interaction data.
To generate these data we use a function included in the package that served as backbone for the evaluations in the manuscript.
The key settings for data generation are the arguments for the magnitude of individual- and dyad-level variation (`indi_sd=` and `dyad_sd=`).
Both values were chosen to be fairly large.
We simulate two behaviors, one a count behavior and the second a proportion.
Both also have their own observation effort specified, varying between 5 and 50 'hours' for the count behavior, and between 5 and 100 'trials' for the proportion\footnote{Note that the proportion is actually also coded as a count, i.e. `number of successes', which in combination with the `trials' provided results in a proportion.}.
The behavior intercepts are both set to -3, which corresponds to a fairly low baseline for each behavior\footnote{For a count variable an intercept of $-3$ corresponds to a rate of $0.05$ events per 1 unit of observation effort ($exp(-3) = 0.0498$). For the proportion a baseline of $-3$ corresponds to probability of success in a single `trial' of $0.05$ ($\textrm{logit}^{-1}(-3) = 0.0474$)}.
Recall that these intercepts represent the expected interactions for an average dyad composed of two average individuals.

```{r}
set.seed(1)
d <- generate_data(n_ids = 14, n_beh = 2,
                   behav_types = c("count", "prop"),
                   indi_sd = 1.2, dyad_sd = 0.8,
                   beh_intercepts = c(-1, -1),
                   prop_trials = c(5, 100),
                   count_obseff = c(5, 20))
```

```{r, eval=FALSE, echo=FALSE}
max(d$processed$interaction_matrices[[1]])
```


The output of `generate_data()` is already pre-processed to simplify the quantitative evaluations presented in the manuscript.
In order to make this example somewhat more realistic, we extract data from it that reflects of what one would typically start with, i.e. interaction matrices and dyadic observation effort (as in the quick start example above).
We also label the individuals with letters, which is helpful down the line to keep track of them\footnote{Stan only works with numeric data so there is no way in Stan to track individuals by their id. Instead, we keep track of ids indirectly, by storing them in the Stan data with a little trick (named vectors).}

```{r}
m1 <- d$processed$interaction_matrices[[1]]
m2 <- d$processed$interaction_matrices[[2]]
colnames(m1) <- rownames(m1) <- LETTERS[1:14]
colnames(m2) <- rownames(m2) <- LETTERS[1:14]

o1 <- m1 - m1
o1[d$standat$dyads_navi] <- d$input_data$obseff[, 1]
o2 <- m2 - m2
o2[d$standat$dyads_navi] <- d$input_data$obseff[, 2]
```


```{r, echo=FALSE, eval=TRUE, fig.width=7, fig.height=4.2, fig.cap="\\label{fig:ex2_mats}Interaction matrices of two behaviors (top row), and two observation effort matrices (bottom row). Only a subset of six individuals is displayed.", out.width="60%", fig.align='center'}
par(mar = c(0, 1.6, 2.6, 0), family = "serif", cex = 0.7, mfrow = c(2, 2))
sel <- 7:12
EloSteepness:::plot_matrix(m1[sel, sel], greyout = 0)
title(main = "interactions 1", line = 1.6)
EloSteepness:::plot_matrix(m2[sel, sel], greyout = 0)
title(main = "interactions 2", line = 1.6)
EloSteepness:::plot_matrix(round(o1[sel, sel], 1), greyout = 0)
title(main = "effort 1", line = 1.6)
EloSteepness:::plot_matrix(o2[sel, sel], greyout = 0)
title(main = "effort 2", line = 1.6)
```

So we end up with two interaction matrices, and with two matrices for observation effort.
For the first behavior, the interactions are counts while the observation effort is continuous (think observation time) (left column in figure \ref{fig:ex2_mats}).
For the second behavior, the interactions are also counts, but the observation effort is also recorded as counts, which results in a proportion (right column in figure \ref{fig:ex2_mats}).

## The first behavior

Although we generated two behaviors above, we start with one.
The first step converts the dyadic data (and observation effort) into a format that can be passed to Stan.
If we supply interaction data and observation effort, we need to make sure that the two matrices match, i.e. they need to have the same dimensions and they need to be ordered in the same way.
The easiest way of achieving this is to use column and row names.
Currently, there is a rudimentary function `check_data()` to check for consistency in the data, which should flag some (but likely not all) problems in the data.

```{r, eval = TRUE}
check_data(mats = list(groom = m1),
           behav_types = c("count"),
           obseff = list(o1))
```

Now we prepare the data.
We require two lists, one for the interaction matrix and one for the observation effort.
Ideally, we name the interaction matrix when creating the list (`mats = list(groom = m1)`), so that later in any output we can find back the corresponding values by referencing the name(s) supplied here (rather than by their cryptic Stan variable names).
If the interaction matrix is not named, the function will make up names (like `behav_A`).
The observation effort list does not need to be named.

As far as specifying the `behav_types=` goes:
This determines which statistical model will be applied to a given interaction matrix.
Table \ref{tab:dat_types} lists the possibilities.

\begin{table}[b]
\centering
\caption{Interaction types and matching observation effort specification supported in \texttt{basomo}.}
\label{tab:dat_types}
\begin{tabular}{p{3cm}p{4cm}p{4cm}}
\hline
name in \texttt{basomo} & interactions & observation effort\\ 
\hline
\texttt{count} & frequencies, counts (integers) & positive continuous\\ 
\texttt{prop} & counts (integers), `successes' & counts (integers), `trials'\\ 
\texttt{dur\_beta} & continuous proportion (between 0 and 1) & NA\\ 
\texttt{dur\_gamma} & positive continuous & positive continuous\\ 
\hline
\end{tabular}
\end{table}




```{r}
sdat1 <- make_stan_data_from_matrices(mats = list(groom = m1),
                                      behav_types = c("count"),
                                      obseff = list(o1))
```

So now we can fit the model.
As mentioned in the first example, the only required argument for `sociality_model()` is the prepared data (`sdat1` here).
Additional arguments pertain to setting parameters for `cmdstanr`'s `$sample()` method, which is what is running under the hood.
In this example, I actually already used one such argument, `seed=16`, which should result in a warning message about divergent transitions.\footnote{The seed argument just fixes the generation of random numbers to a replicable state. In this particular example I chose it such that we actually do get divergent transitions, but it doesn't \textit{cause} the divergent transitions. Try it out with different values and you will see that sometimes divergent transitions occur and sometimes they don't.}

```{r fit_example_1_1_display, eval = FALSE, echo=FALSE}
fit1 <- sociality_model(standat = sdat1, refresh = 0, parallel_chains = 4, seed = 16)
```

```{r fit_example_1_1, eval = FALSE}
fit1 <- sociality_model(standat = sdat1, seed = 16)
```

Divergent transitions occur quite regularly, but usually can be resolved by setting a higher value to `adapt_delta` (away from its default 0.8).
So let's do this, and in the process also change a few more settings, just to illustrate how this works.

```{r fit_example_1_2, cache=TRUE, include=FALSE}
fit1 <- sociality_model(standat = sdat1, seed = 16,
                        adapt_delta = 0.9,
                        parallel_chains = 4, iter_warmup = 1200)
```

```{r fit_example_1_2_display, cache=FALSE, eval = FALSE}
fit1 <- sociality_model(standat = sdat1,
                        seed = 16,
                        adapt_delta = 0.9,
                        parallel_chains = 4,
                        iter_warmup = 1200)
```

So here we indicate that we want to run four chains in parallel and also increase the number of warm-up iterations.
We keep the same random seed.
Now the model should sample fine without warnings.
Check out the documentation for `sociality_model()` to see other frequently helpful options.

The resulting object is a list with two items, which combines the Stan data used to fit the model and the `cmdstanr` model environment.
If you are familiar with `cmdstanr` models, you can directly access the model environment via

```{r, eval=FALSE}
fit1$mod_res
```

and take it from there.
If you are unfamiliar with such objects, the `basomo` package provides a few helper functions to visualize and summarize the results.

Before looking at the results of interest in detail, we look at the model summary.

```{r}
summary(fit1)
```

This just gives an overview over the estimated model, and if there were issues with fitting, these will be reported here too.

## Model checks

Next, we go to some visual model checks.
Because the underlying observed data are discrete (counts in this first example), we use histograms provided by `pp_model()` to look at predictions from the model.\footnote{for continuous data, use density plots from `pp\_model\_dens()`.}
One thing to be careful about for these kinds of plot is the width of bins for the histogram.
Here I chose 10 bins, ranging from 0 to 100 (`r max(sdat1$interactions[, 1])` being the value for the dyad that interacted the most).
Actually, I set the bins as a sequence from $-0.5$ to $99.5$ in steps of $10$ (`bins <- seq(-0.5, 99.5, by = 10)`) to be sure that we cover all values from $0$ through $9$ in the first bin, from $10$ to $19$ in the second and so forth.

```{r, echo=2:10, fig.width=5, fig.height=3, out.width="50%", fig.align='center', fig.cap="\\label{fig:pp01}Posterior predictive checks for the entire group."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
bins <- seq(-0.5, 199.5, by = 10)
pp_model(mod_res = fit1, xbreaks = bins, 
         xlab = "count of behaviour 1", ylab = "frequency")
axis(2, las = 1, cex.axis = 0.8)
```

Figure \ref{fig:pp01} shows the observed data (for all dyads) as histogram. 
For example, there are slightly less than 70 dyads with at most 10 interactions (`r sum(sdat1$interactions[,1] <= 10)` actually).
The added circles represent the median predicted frequencies for each bin from all model draws (4000 using the default settings).
Ideally, these median predictions match the observed frequencies closely, and here they do reasonably well.
If the model had difficulties to fit the observed data, we would see predictions that are far from the actually observed data.

One thing to note here is that the model predicts values that were never actually observed.
For example, no dyad was observed interacting between 40 and 50 times (no bar in this bin).
The model however predicts such values to occur sometimes (median prediction for that bin is 1 interaction).
This is not problematic per se and we will come back to this below.

In addition the plot shows how far the predictions spread around the medians (aka uncertainty).
The vertical lines represent 89% credible intervals.
These intervals were also calculated from all model draws.
We would like to see that these intervals are not too wide.
Here they are not, but if they were this just illustrates that our model has not enough information to really make accurate predictions (or that we really have a bad model fit).

One other thing to keep in mind when looking at these predictions is that the assessment of how good the model fit is depends a lot on the resolution, i.e. the bin width.
In figure \ref{fig:pp01} we used bins of width 10, i.e. we counted dyads that interacted between 0 and 9 times, between 10 and 19 times and so forth.
For figure \ref{fig:pp02}a we use a finer resolution\footnote{Note that we want to explicitely separate the occurences of 0 and 1 interactions, which is why we set the break points for the bin widths to start at -0.5.}.
Here we see some (slight) discrepancies between observed and predicted values.
For example, we observed `r table(sdat1$interactions)["4"]` dyads with 4 interactions, but our model predicted a median of 7 dyads.
We also observed `r table(sdat1$interactions)["7"]` dyads with 7 interactions, but the model predicted a median of 4 dyads. 
In this particular example, I would not consider this problematic though because in absolute terms these cases of under- or over-estimation are of small magnitude (we are not predicting 200 dyads when we observed 3).

At the other extreme end, we could use a very small bin width (figure \ref{fig:pp02}b).
Here the median predictions match the observed data almost perfectly.
I would not use this plot to conclude that my model fits the data well, but rather I put it here to illustrate the consequences of choosing the bin width.

One last thing these two extreme cases illustrate is how the amount of uncertainty around the median predictions varies with the chosen bin width. 
For the smaller bin width (figure \ref{fig:pp02}a), the intervals are fairly wide, while the intervals for the very crude bins (figure \ref{fig:pp02}b) are so small that they are not even visible in the figure.

Which bin width to choose is a matter of taste, mostly (I suppose). 
Bins that are too large won't be useful.
The goal is to a get a grasp on how well the model performs.
My personal rule of thumb is to use between 10 to 20 bins (equally distributed along the range of data).


```{r, echo=2:20, fig.width=8, fig.height=3, out.width="50%", fig.align='center', fig.cap="\\label{fig:pp02}Posterior predictive checks for the entire group with different resolutions along the x-axis (compare to figure \\ref{fig:pp01}). Note that the first plot is cut at 20 along the x-axis for visual purposes."}
par(family = "serif", mgp = c(1.5, 0.5, 0),
    mar = c(2.5, 2.5, 0.5, 0.5), mfrow = c(1, 2))

bins <- seq(-0.5, 199.5, by = 1)
pp_model(mod_res = fit1, xlab = "count of behaviour 1", ylab = "frequency", 
         xbreaks = bins, xlim = c(0, 20))
axis(2, las = 1, cex.axis = 0.8)

bins <- seq(-0.5, 199.5, by = 50)
pp_model(mod_res = fit1, xlab = "count of behaviour 1", ylab = "frequency", 
         xbreaks = bins, xlim = c(0, 200))
axis(2, las = 1, cex.axis = 0.8)
```

To dig into model performance even further, we can stratify the predictions by individual.
In other words, we can select a specific individual and look at observed and predicted interactions for all dyads that this specific individual was part of.
We use the same function (`pp_model()`), but supply the argument `selected_id=`.
Figure \ref{fig:pp03} shows the results for the first three individuals (A through C).

The building parts of these plots are the same as above. 
There are at least two things to note here.
First, the uncertainties around the predictions might, on a first glance, appear larger than in the group level plots above.
But note the range along the vertical axis: we are talking about much smaller number of dyads (each plot only reflects the portion of dyads the selected individual is part of), so in absolute terms these uncertainties are very similar to the ones in the group level plots.
Second, recall that the model also predicts interaction numbers that were not actually observed.
Individual E for example, was part of a dyad that interacted `r max((m1 + t(m1))["E", ])` times, which is an extreme value for that particular individual\footnote{It could be that this dyad has a very strong affinity, or both dyad members were extremely gregarious, or we simply observed this dyad for a long time.}.
Now, the model predicts also values of 150 up to 200 interactions for that dyad.
The key point here is that this not problematic, but just illustrates that the model is not predicting *perfectly* but in any case does a good job at predicting that this dyad interacts substantially more than the other dyads of individual E.


```{r, echo=3:30, fig.width=8, fig.height=2.4, fig.cap="\\label{fig:pp03}Posterior predictive checks for selected individuals."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
par(mfrow = c(1, 3))

bins <- seq(-0.5, 199.5, by = 2)
pp_model(mod_res = fit1, selected_id = "A", xbreaks = bins, xlim = c(-1, 50),
         xlab = "count of behaviour 1", ylab = "frequency")
legend("top", "A", bty = "n")
axis(2, las = 1)

bins <- seq(-0.5, 199.5, by = 5)
pp_model(mod_res = fit1, selected_id = "B", xbreaks = bins, xlim = c(-1, 200), 
         xlab = "count of behaviour 1", ylab = "frequency")
legend("top", "B", bty = "n")
axis(2, las = 1)

bins <- seq(-0.5, 199.5, by = 10)
pp_model(mod_res = fit1, selected_id = "E", xbreaks = bins, xlim = c(-1, 200),
         xlab = "count of behaviour 1", ylab = "frequency")
legend("top", "E", bty = "n")
axis(2, las = 1)
```

```{r, echo=FALSE, eval=FALSE}
m1 + t(m1)

id_index <- which(names(sdat1$id_codes) == "C")
navi <- sdat1$dyads_navi
dyad_index <- which(apply(navi, 1, function(xx) id_index %in% xx))
p <- as.matrix(fit1$mod_res$draws(variables = "interactions_pred",
                             format = "matrix"))
p <- p[, grepl(paste0(",", 1, "\\]", collapse = ""), colnames(p))]

p <- p[, dyad_index, drop = FALSE]


```


Finally, there are even more ways to illustrate how well or badly the model predicts our observations.
We could for example look at the maximum value of interactions predicted. 
A good model would on average predict maximum interactions that are close on average to the observed data.
So we extract for each model draw the number of interactions for the dyad that interacted the most.
Figure \ref{fig:pp04}a shows a histogram of these (4,000 by default) maximum values.
The red line represents what we observed in the actual data.
The conclusion here is that our model performs well with respect to predicting interaction patterns that are consistent with our observed data.
Figure \ref{fig:pp04}b shows the same approach for the inter-quartile range. 

At the moment, there are only a handful of these summary statistics defined in `pp_model_stat()` (`"mean"`, `"min"`, `"max"`, `"iqr"`, and `"range_width"`).


```{r, echo=2:4, fig.width=8, fig.height=3, out.width="80%", fig.align='center', fig.cap="\\label{fig:pp04}Posterior predictive checks for the entire group using summary statistics."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5),
    mfrow = c(1, 2))

pp_model_stat(mod_res = fit1, stat = "max", main = "", xlab = "max interactions")
pp_model_stat(mod_res = fit1, stat = "iqr", main = "", xlab = "IQR of interactions")
```

If you want to dig deeper into to the concept of visual model checks, @gabry2019 is a good starting point.

## Visual model results

Visualization of the key results works the same way as in the first example.
`sociality_plot()` produces a plot of the posteriors of the variation (the estimated SD, actually) of the two sociality components (figure \ref{fig:xhfegr}).
There are number of arguments to the function in addition to the one required (`mod_res = fit1`), and below I specify them at their defaults. 
Their meaning should be self-explanatory.
Since this model is based on simulated data, we can also add the ground truth for both parameters, which are the vertical lines at $`r d$input_data$indi_sd`$ and $`r d$input_data$dyad_sd`$.

```{r, include=FALSE}
d$input_data$indi_sd
d$input_data$dyad_sd
```

```{r, eval = TRUE, echo=2:3, fig.width=7, fig.height=4, out.width="50%", fig.cap="\\label{fig:xhfegr}Individual and dyadic variation components."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
sociality_plot(mod_res = fit1, do_legend = TRUE, do_dyadic = TRUE, do_indi = TRUE)
abline(v = d$input_data$indi_sd) # value used during data generation
abline(v = d$input_data$dyad_sd) # value used during data generation
```


We can also plot the posteriors of the estimated individual gregariousness and dyadic affinity values with `ridge_plot()` (figure \ref{fig:ubsglsdg}).
Note that these plots become cluttered quickly depending on the number of individuals in the data set, and this is especially true for plotting dyadic estimates.


```{r, eval = TRUE, echo=2:10, fig.width=8, fig.height=5, out.width="80%", fig.align='center', fig.cap="\\label{fig:ubsglsdg}Posteriors for individual gregariousness and dyadic affinity."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5), mfrow = c(2, 2))
ridge_plot(fit1)
ridge_plot(fit1, sel_subset = c("A", "C"))
ridge_plot(fit1, greg = FALSE, sel_subset = c("F_@_J", "M_@_N", "D_@_L"), vert_exp = 10)
ridge_plot(fit1, greg = FALSE, sel_subset = c("F", "A_@_B", "D_@_L"), vert_exp = 10)
```



## Numeric model results

There are basically two ways of getting numerical output of the model results.
`model_summary()` returns summaries of the posteriors, and `extract_samples()` extracts the entire posteriors of the primary parameters.


`model_summary()` as used below returns a data frame with point estimates (mean and median) and some measures of uncertainty and sampling diagnostics.
The returned parameters are grouped in the `$categ` column as either `sd` (standard deviations of gregariousness and pairwise affinity), `intercept` (intercepts for all behaviors), `indi_vals` (gregariousness for each individuals) and `dyad_vals` (pairwise affinity for each dyad), which allows filtering.
The only relevant argument to the function is needed for specifying the quantiles to be returned for each posterior (the default is an 89% interval: `probs = c(0.055, 0.945)`).

```{r, results='hide'}
modsum <- model_summary(fit1, probs = c(0.1, 0.9))
head(modsum)
head(modsum[modsum$categ == "dyad_vals", ])
```

```{r, echo=FALSE}
print(head(modsum), digits = 3)
```

```{r, echo=FALSE}
print(head(modsum[modsum$categ == "dyad_vals", ]), digits = 3)
```


With `extract_samples()`, we can get the full posteriors for each parameter.
Here, the output is a matrix, where each row represents one posterior sample (so the total number of rows is by default $4,000$, i.e. $4$ chains each with $1,000$ samples).
The columns represent the parameters and they are grouped in the same way as in `model_summary()`, i.e. the two standard deviations for gregariousness and affinity, followed by the intercepts for each behavior, followed by the values for each individual and dyad.

```{r, results='hide'}
post <- extract_samples(fit1)
head(post)[, 1:12]
```

```{r, echo=FALSE}
print(head(post[, 1:12]), digits = 2)
```

There is one relevant argument for `extract_samples()` is `what=`, and this determines which parameters are returned. 
The default is to extract all parameters, but this can be changed for example to return only the dyadic affinity values.
For the remaining options, see the function's help file (`?extract_samples`).

```{r, results='hide'}
post <- extract_samples(fit1, what = "dyad_vals")
head(post)[, 1:12]
```

```{r, echo=FALSE}
print(head(post[, 1:12]), digits = 2)
```


## The second behavior

Now we turn to the second behavior, which represents a proportion.
Here, the interaction matrix also contains integers, and crucially, the corresponding observation effort is also recorded as integer values.
In other words, it's only through the combination of interactions and observation effort that we can actually talk about proportions.


Apart from that, everything works the same as in the example(s) above.
We prepare the data and fit the model.

```{r}
sdat2 <- make_stan_data_from_matrices(mats = list(prox = m2),
                                      behav_types = c("prop"),
                                      obseff = list(o2))
```

```{r fit_example_2_1_display, eval = FALSE}
fit2 <- sociality_model(standat = sdat2, seed = 8)
```

```{r fit_example_2_1, cache=TRUE, include=FALSE}
fit2 <- sociality_model(standat = sdat2,
                        refresh = 0, parallel_chains = 4, seed = 8)
```

We now also run posterior predictive checks, but this time we look at densities rather than at histograms (figure \ref{fig:pp0x}).
This makes sense if the quantity we want to display can be seen as continuous.\footnote{Technically, these data are still discrete and not continuous, but we proceed anyway for illustration purposes.}

The function we use is `pp_model_dens()`.
The underlying idea is the same as in `pp_model()`: 
we plot the density of the observed data (red line in the plot) and combine this with densities from a small number of draws representing predictions from the model (the default is `n_draws=20`).
A good model leads to predictions that correspond approximately to the observed data.

```{r, echo=2:10, fig.width=5, fig.height=3, out.width="50%", fig.align='center', fig.cap="\\label{fig:pp0x}Posterior predictive checks for a behavior representing a proportion."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
pp_model_dens(mod_res = fit2, xlab = "count of behaviour 2", ylab = "frequency")
axis(2, las = 1, cex.axis = 0.8)
```

We can also produce a plot of the two sociality components using `sociality_plot()` (figure \ref{fig:ggrashg}).

```{r, echo=2:3, fig.width=7, fig.height=4, out.width="50%", fig.cap="\\label{fig:ggrashg}Individual and dyadic variation components."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5))
sociality_plot(mod_res = fit2)
abline(v = 1.5)
```


## Combining two behaviors 

It becomes really interesting when we model two behaviors simultaneously. 
In other words, we want to estimate the underlying sociality components from two or more interaction matrices.
There is the implicit assumption that such underlying sociality components exist in the first place.

So when preparing the data, we just need to make sure that we combine the two behaviors in question:

```{r}
sdat3 <- make_stan_data_from_matrices(mats = list(groom = m1, prox = m2),
                                      behav_types = c("count", "prop"),
                                      obseff = list(o1, o2))
```

The fitting of the model then works the same way as before.

```{r fit_example_3_1_display, eval = FALSE}
fit3 <- sociality_model(standat = sdat3,
                        refresh = 0, parallel_chains = 4, seed = 3)
```

```{r fit_example_3_1, cache=TRUE, include=FALSE}
fit3 <- sociality_model(standat = sdat3,
                        refresh = 0, parallel_chains = 4, seed = 3)
```

```{r}
summary(fit3)
```


The most important thing to remember when working with more than one behavior is that we have to declare for which behavior we want to make predictions when using `pp_model()` and `pp_model_dens()` (figure \ref{fig:pp05}).

```{r, echo = 2:8, fig.width=8, fig.height=3, out.width="50%", fig.align='center', fig.cap="\\label{fig:pp05}Posterior predictive checks for two behaviors."}
par(family = "serif", mgp = c(1.5, 0.5, 0), mar = c(2.5, 2.5, 0.5, 0.5), mfrow = c(1, 2))

pp_model(mod_res = fit3, xvar = "groom",
         xlab = "grooming frequency", ylab = "frequency")
pp_model_dens(mod_res = fit3, xvar = "prox",
         xlab = "prox frequency", ylab = "frequency")
```


\clearpage

# Extending the model

## More than one behaviour with correlated axes

As mentioned above, when we fit the model to data we make the implicit (or hopefully explicit) assumption that there is exactly one, not two or three, underlying gregariousness axes (and of course exactly one affinity axis (not two or three or ...)).
But this is just an assumption.
We might as well make the assumption that more than one gregariousness axis exists and that they are correlated.
Imagine two behaviors.
Here we could estimate two gregariousness axes, one for each behavior. 
Plus, we can actually investigate whether these multiple axes are correlated with each other.

Let's look at an example with three behaviors, so we have three gregariousness axes and three affinity axes.
Since there are three behaviors there are also three correlations between them (G1 with G2, G1 with G3 and G2 with G3 (and the same for affinity)).
We simulate the data so we can actually be sure what the true correlations are.
Without going into too much detail: we simulate interactions such that the correlations are $-0.4$, $0.4$, and $0.6$ for gregariousness.
We also set correlations for affinity.
These values are handed over to `generate_data` via SD/correlation matrices, which also encode the ground truth SD values in the diagonal.

```{r}
corgreg12 <- -0.4
corgreg13 <- 0.4
corgreg23 <- 0.6

gregmat <- matrix(c(0.9, corgreg12, corgreg13,
                    corgreg12, 1.5, corgreg23,
                    corgreg13, corgreg23, 1.2),
                  ncol = 3, byrow = TRUE)

coraffi12 <- -0.6
coraffi13 <- 0.3
coraffi23 <- -0.4
affimat <- matrix(c(0.5, coraffi12, coraffi13,
                    coraffi12, 1.2, coraffi23,
                    coraffi13, coraffi23, 1.2),
                  ncol = 3, byrow = TRUE)

set.seed(1)
ex <- generate_data(n_ids = 18, n_beh = 3,
                    behav_types = c("count", "prop", "prop"),
                    indi_sd = gregmat,
                    dyad_sd = affimat,
                    beh_intercepts = c(1.4, -0.7, -1.7), exact = TRUE)
```

```{r, echo = FALSE, eval = FALSE}
# s1 <- ex$standat
# s2 <- ex$standat
# s2$n_cors <- 0
```


The `generate_data()` function actually prepares the data directly, but we take the detour and create everything by hand.
Here are the interactions matrices

```{r}
b1 <- ex$processed$interaction_matrices[[1]]
b2 <- ex$processed$interaction_matrices[[2]]
b3 <- ex$processed$interaction_matrices[[3]]
```

and the observation effort matrices\footnote{The matrices with the observation effort are somewhat simple looking, because during the data generation we assume equal observation effort over all dyads}.

```{r}
o1 <- ex$processed$obseff_matrices[[1]]
o2 <- ex$processed$obseff_matrices[[2]]
o3 <- ex$processed$obseff_matrices[[3]]
```

Now we can prepare the data for usage with `sociality_model()`.
In fact, we want to fit two models.
One should mirror the true structure of the data-generating process and one should be oblivious and keep assuming single gregariousness and affinity axes.


```{r}
s1 <- make_stan_data_from_matrices(mats = list(b1 = b1, b2 = b2, b3 = b3), 
                                   behav_types = c("count", "prop", "prop"), 
                                   obseff = list(o1, o2, o3))
```

And to indicate that we want to fit separate axes and correlations we set `correlation = TRUE`:

```{r}
s2 <- make_stan_data_from_matrices(mats = list(b1 = b1, b2 = b2, b3 = b3),
                                   behav_types = c("count", "prop", "prop"),
                                   obseff = list(o1, o2, o3),
                                   correlations = TRUE)
```


With these two data objects we can now fit the two models.


```{r fit_example_cor_correct, cache=TRUE, results='hide'}
# incorrect model
r1 <- sociality_model(s1, parallel_chains = 4, refresh = 0,
                      show_exceptions = FALSE, seed = 42)
```

```{r fit_example_cor_incorrect, cache=TRUE, results='hide'}
# correct model
r2 <- sociality_model(s2, parallel_chains = 4, refresh = 0,
                      show_exceptions = FALSE, seed = 42)
```


```{r}
summary(r1)
```

```{r}
summary(r2)
```

Those results for the 'correct' model look alright (the estimated correlations are close to the true values and so are the SDs and intercepts).
In contrast, the incorrect model doesn't even get the SDs right (and how could it, the model only estimates one individual and one dyadic axis, no three).
But how do we know which model to use?
In reality we don't know the true model.
The obvious choice would be to use some kind of model comparison, which should at least let us drop the worst models.
But that's yet to be implemented.

For now, let's look at posterior predictive checks.
And there it becomes clear that there is a problem with the second model (figures \ref{fig:ppcheckcormodel1} and \ref{fig:ppcheckcormodel2}), while the first model seems to do a reasonable job.


```{r, echo=FALSE, fig.width=8, fig.height=3.2, out.width="80%", fig.align='center', fig.cap="\\label{fig:ppcheckcormodel1} Top row: correct model, bottom row: incorrect model. Histogram is the observed data."}
par(mfrow = c(2, 3), family = "serif", mar = c(3, 2, 1, 1))
bamoso::pp_model(r2, xvar = 1)
bamoso::pp_model(r2, xvar = 2)
bamoso::pp_model(r2, xvar = 3)

bamoso::pp_model(r1, xvar = 1, main = "behavior 1")
bamoso::pp_model(r1, xvar = 2, main = "behavior 2")
bamoso::pp_model(r1, xvar = 3, main = "behavior 3")
```


```{r, echo=FALSE, fig.width=8, fig.height=3.2, out.width="80%", fig.align='center', fig.cap="\\label{fig:ppcheckcormodel2} Top row: correct model, bottom row: incorrect model. Red is the observed value."}
par(mfcol = c(2, 3), family = "serif", mar = c(3, 2, 1, 1), xaxs = "i")
bamoso::pp_model_stat(r2, 1, "max", xlim = c(20, 70), main = "")
bamoso::pp_model_stat(r1, 1, "max", xlim = c(20, 70), main = "max of behaviour 1")

bamoso::pp_model_stat(r2, 2, "iqr", xlim = c(20, 50), main = "")
bamoso::pp_model_stat(r1, 2, "iqr", xlim = c(20, 50), main = "IQR of behaviour 2")

bamoso::pp_model_stat(r2, 3, "iqr", xlim = c(10, 30), main = "")
bamoso::pp_model_stat(r1, 3, "iqr", xlim = c(10, 30), main = "IQR of behaviour 3")

```


And finally, let's look at some results plots of the model with the correlations.


```{r, fig.width=8, fig.height=1.8, out.width="80%", fig.align='center', fig.cap="Posteriors of estimated correlations. Vertical lines indicate true values (input values for simulated data)."}
par(mfcol = c(1, 3), family = "serif", mar = c(3, 2, 1, 1), xaxs = "i")
sociality_plot(r2, which_cor = c(1, 2), xlim = c(-1, 1))
abline(v = rep(gregmat[1, 2], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[1, 2], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))

sociality_plot(r2, which_cor = c(1, 3), xlim = c(-1, 1))
abline(v = rep(gregmat[1, 3], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[1, 3], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))

sociality_plot(r2, which_cor = c(2, 3), xlim = c(-1, 1))
abline(v = rep(gregmat[2, 3], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[2, 3], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))
```

```{r, fig.width=8, fig.height=1.8, out.width="80%", fig.align='center', fig.cap="Posteriors of estimated differentiation in gregariousness and affinity. Vertical lines indicate true values (input values for simulated data)."}
par(mfcol = c(1, 3), family = "serif", mar = c(3, 2, 1, 1), xaxs = "i")
sociality_plot(r2, which_beh = 1, xlim = c(0, 3))
abline(v = rep(gregmat[1, 1], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[1, 1], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))

sociality_plot(r2, which_beh = 2, xlim = c(0, 3))
abline(v = rep(gregmat[2, 2], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[2, 2], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))

sociality_plot(r2, which_beh = 3, xlim = c(0, 3))
abline(v = rep(gregmat[3, 3], 2), col = c("darkgrey", "#3B99B1B3"), lwd = c(3.5, 2))
abline(v = rep(affimat[3, 3], 2), col = c("darkgrey", "#EACB2BB3"), lwd = c(3.5, 2))
```


## Correlation example with empirical data

TBD.


more to be written

\clearpage

# Prior predictive simulations

For now, the priors for most parameters in the model are hardcoded in the Stan model code.\footnote{The exceptions are the priors for the behavioural intercepts, which are derived from the observed interactions.}
This is likely to change in the future, but for now the priors for the standard deviations of the individual gregariousness and the dyadic affinity seem to work reasonably well across a range of situations.

The simulations in this section just demonstrate what happens if we use only the priors to estimate the parameters of interest.
Actually, we still need the data because we require the observation effort (if present) and the interactions (to determine the priors for the behavioral intercepts).

```{r, eval = FALSE}
set.seed(123)
x <- generate_data(n_ids = 33, n_beh = 1, behav_types = "count",
                   indi_sd = 0.3, dyad_sd = 0.3, 
                   beh_intercepts = 0.2)

mats <- list(x$processed$interaction_matrices[[1]])
obseff <- list(x$processed$obseff_matrices[[1]])
dat <- make_stan_data_from_matrices(mats = mats, obseff = obseff, 
                                    behav_types = "count")


# prior only ('null model')
rnull <- sociality_model(dat, prior_sim = TRUE,
                         parallel_chains = 4, refresh = 0, 
                         iter_sampling = 247, seed = 42)
# actual model
r <- sociality_model(dat, prior_sim = FALSE, 
                     parallel_chains = 4, refresh = 0, 
                     iter_sampling = 247, seed = 42)
```

In the prior only model, we expect to see an intercept of 0.2 (as we set in the data generation step).
The estimates for both variance components should be around 0.5 (which is the mean of the prior distribution (exponential(2))).
So that works quite well.

```{r, eval = FALSE}
rnull$mod_res$summary(c("beh_intercepts", "indi_soc_sd", "dyad_soc_sd"))
```

When we condition on the actual data, we still see the intercept of around 0.2 .
But this time we recover the actual parameter values (0.3 and 0.3) (reasonably well, (I'm very generous with rounding here)).
If you are not impressed, simulate again with a larger number of individuals.

```{r, eval = FALSE}
r$mod_res$summary(c("beh_intercepts", "indi_soc_sd", "dyad_soc_sd"))
```


We can also visualize these simulations.
We do so by using the prior-only model to make predictions about the behavior.
These predictions only 'know' two things from the observed data: the mean of the interaction numbers and the observation effort (if any).
And now we predict interactions.
The plot below shows this in a quick and dirty way\footnote{Confusingly, it also displays the observed data in red..., this is gonna be turned off sometime soon.}

```{r, eval = FALSE}
pp_model_dens(rnull, xlim = c(0, 20), ylim = c(0, 0.5), n_draws = 20, xadjust = 2)
pp_model_dens(r1, ylim = c(0, 0.5), xlim = c(0, 50), n_draws = 100, xadjust = 2)
```

```{r, eval = FALSE, echo=FALSE}
p <- rnull$mod_res$draws("interactions_pred", format = "draws_matrix")
# p <- p[1:20, ]
hist(p[1, ])
hist(p[2, ])
hist(log(apply(p, 1, max)))
```




\clearpage

# Intuitions

In this section, I just want to demonstrate a few intuitions I have/had and see how they play out.

## Uncertainty and observation effort

to be written





## Correlated axes 1

We assume that when we have two or more behaviors, that they have one underlying affinity and gregariousness axis.
This should be essentially the same as assuming that we have one axis per behavior and that these axes perfectly correlated with each other.
So if we generate a data set of interactions of two behaviors and we generate them with two axes that strongly correlate, I would expect no major difference in inference between a model that incorporate the correlations and one that doesn't.



```{r intui_cor1_models, cache=TRUE, echo=TRUE, eval = FALSE ,include=FALSE}
cors_dyad <- matrix(c(1.5, 0.95, 0.95, 0.5), ncol = 2)
cors_indi <- matrix(c(1, 0.95, 0.95, 1), ncol = 2)

set.seed(123)
x <- generate_data(n_ids = 18, n_beh = 2,
                   beh_intercepts = c(-1.5, 0.5), 
                   behav_types = c("prop", "prop"), 
                   indi_sd = cors_indi, 
                   dyad_sd = cors_dyad)

dat_withcor <- x$standat
r_withcor <- sociality_model(dat_withcor, parallel_chains = 4, seed = 1,
                            iter_warmup = 500, iter_sampling = 500)

dat_nocor <- x$standat
dat_nocor$n_cors <- 0
r_nocor <- sociality_model(dat_nocor, parallel_chains = 4, seed = 1, 
                          iter_warmup = 500, iter_sampling = 500)

```


```{r, echo=TRUE, eval=FALSE}
post_withcor <- extract_samples(r_withcor, what = c("indi_sd", "dyad_sd"))
post_nocor <- extract_samples(r_nocor, what = c("indi_sd", "dyad_sd"))
apply(post_withcor, 2, median)
apply(post_nocor, 2, median)

```


So that didn't work out as I expected...
What I overlooked (duh) was that this intuition can only be correct if the SD of the two affinity axes is identical (and not only the correlation large).
So if we look at the results of the models this becomes apparent quickly.


```{r, echo=TRUE, eval = FALSE, fig.width=8, fig.height=3.4, out.width="80%", fig.cap=figcap, fig.align='center'}
par(family = "serif", mgp = c(1.5, 0.5, 0), las = 1, mar = c(3, 3, 1, 1), tcl = 0)
par(mfrow = c(2, 3))

p <- model_summary(r_withcor)
pa <- p[p$categ == "dyad_vals" & p$beh == "behav_A", "median"]
pb <- p[p$categ == "dyad_vals" & p$beh == "behav_B", "median"]

p <- model_summary(r_nocor)
ps <- p[p$categ == "dyad_vals", "median"]

plot(x$input_data$dyad_soc_vals[, 1], pa, asp = 1, xlab = "true values (behavior 1)",
     ylab = "estimated (cor model, behav 1)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)
plot(x$input_data$dyad_soc_vals[, 1], pb, asp = 1, xlab = "true values (behavior 1)",
     ylab = "estimated (cor model, behav 2)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)
plot(x$input_data$dyad_soc_vals[, 1], ps, asp = 1, xlab = "true values (behavior 1)",
     ylab = "estimated (model without cor)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)

plot(x$input_data$dyad_soc_vals[, 2], pa, asp = 1, xlab = "true values (behavior 2)",
     ylab = "estimated (cor model, behav 1)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)
plot(x$input_data$dyad_soc_vals[, 2], pb, asp = 1, xlab = "true values (behavior 2)",
     ylab = "estimated (cor model, behav 2)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)
plot(x$input_data$dyad_soc_vals[, 2], ps, asp = 1, xlab = "true values (behavior 2)",
     ylab = "estimated (model without cor)",
     xlim = c(-4, 4), ylim = c(-4, 4), yaxs = "i", xaxs = "i")
abline(0, 1, lty = 2)
```






## Correlated axes 2

```{r, eval = FALSE}
set.seed(123)
cors_indi <- matrix(c(1.3, 0.7, 0.7, 1.3), ncol = 2)
cors_dyad <- matrix(c(0.3, 0.3, 0.3, 0.3), ncol = 2)
x <- generate_data(n_ids = 12, n_beh = 2,
                   beh_intercepts = c(0.5, 0.5), behav_types = c("count", "count"),
                   indi_sd = cors_indi, dyad_sd = cors_dyad)
s <- x$standat
r <- sociality_model(s, parallel_chains = 4, seed = 1, adapt_delta = 0.85,
                     show_exceptions = FALSE, refresh = 0,
                     iter_warmup = 500, iter_sampling = 1000)
summary(r)
s1 <- extract_samples(r, "indi_vals", axis = 1)
s2 <- extract_samples(r, "indi_vals", axis = 2)


plot(0, 0, type = "n", xlim = c(-5, 5), ylim = c(-5, 5), asp = 1)

for (i in ix) {
  points(s1[, i], s2[, i], pch = ".", cex = 0.5, col = grey(0.1, 0.2))
}
for (i in ix) {
  zz <- MASS::kde2d(s1[, i], s2[, i], n = 51)
  pdata <- contourLines(zz, levels = quantile(zz$z, 0.95))
  polygon(pdata[[1]]$x, pdata[[1]]$y, border = "red", lwd = 1.6)
}
points(colMeans(s1), colMeans(s2), pch = 4, col = "red", lwd = 3)

```







# Simple model with `brms`

The simplest implementation of the model, i.e. with interactions from one behavior, can also be fit with `brms` [@buerkner2017] but requires some post fitting adjustment.
In order to bring the individual level parameters (gregariousness SD and individual gregariousness values) to the same scale, we need to multiply by $\sqrt{0.5}$.
The code below demonstrates this without showing actual results.
The `brms` estimates correspond closely to the ones from `bamoso`.

```{r, eval = FALSE}
# set.seed(1)
x <- generate_data(n_ids = 17,
                   n_beh = 1, 
                   beh_intercepts = c(0.2),
                   behav_types = c(approach = "count"), 
                   count_obseff = 20,
                   indi_sd = 0.5, 
                   dyad_sd = 0.8)

# interaction matrices
imats <- list(approach = x$processed$interaction_matrices[[1]])
# observation effort matrices
omats <- list(x$processed$obseff_matrices[[1]])
# data object
standat <- make_stan_data_from_matrices(mats = imats, 
                                        behav_types = c("count"), 
                                        obseff = omats)

# data object for brms call (a data.frame)
dyads <- x$input_data$dyad_data[, c("dyad", "id1", "id2")]
dyads$approach <- imats[[1]][as.matrix(dyads[, 2:3])]
dyads$obseff <- omats[[1]][as.matrix(dyads[, 2:3])]
```


<!-- priors <- c(set_prior("exponential(2)", class = "sd", lb = 0)) -->

```{r, eval = FALSE}
library(brms)
bres <- brm(approach ~ offset(log(obseff)) + (1|dyad) + (1|mm(id1, id2)),
            data = dyads, family = poisson,
            backend = "cmdstanr", cores = 4)
bamres <- sociality_model(standat, parallel_chains = 4, show_exceptions = FALSE)
```


```{r, eval = FALSE}
plot(0, 0, xlim = c(0, 1.5), ylim = c(0, 12), yaxs = "i", type = "n", 
     xlab = "estimate", ylab = "density")
cols <- adjustcolor(hcl.colors(3, palette = "zissou"), 0.5)

res_brms <- rstan::extract(bres$fit, "sd_mmid1id2__Intercept")$sd_mmid1id2__Intercept
res_brms <- res_brms * sqrt(0.5)
res_bam <- extract_samples(bamres, "indi_sd")[, 1]
polygon(density(res_brms), col = cols[1])
polygon(density(res_bam), col = cols[1])
abline(v = c(median(res_brms), median(res_bam)))

res_brms <- rstan::extract(bres$fit, "sd_dyad__Intercept")$sd_dyad__Intercept
res_bam <- extract_samples(bamres, "dyad_sd")[, 1]
polygon(density(res_brms), col = cols[2])
polygon(density(res_bam), col = cols[2])
abline(v = c(median(res_brms), median(res_bam)))

res_brms <- rstan::extract(bres$fit, "b_Intercept")$b_Intercept
res_bam <- extract_samples(bamres, "beh_intercepts")[, 1]
polygon(density(res_brms), col = cols[3])
polygon(density(res_bam), col = cols[3])
abline(v = c(median(res_brms), median(res_bam)))

legend("topright", legend = c("greg. SD", "affi. SD", "intercept"),
       col = cols, pch = 15, bty = "n", ncol = 1, cex = 0.6)
```



```{r, eval = FALSE}
par(mfrow = c(1, 2), mgp = c(1.7, 0.3, 0), las = 1)

# gregariousness (needs adjustment)
x <- ranef(bres)$mmid1id2[, , 1][, "Estimate"]
x <- x * sqrt(0.5)
y <- colMeans(extract_samples(bamres, "indi_vals"))
plot(x, y, xlab = "brms estimates (adjusted)", ylab = "bamoso estimates")
abline(0, 1)

# affinity (no adjustment)
x <- ranef(bres)$dyad[, , 1][, "Estimate"]
y <- colMeans(extract_samples(bamres, "dyad_vals"))
plot(x, y, xlab = "brms estimates", ylab = "bamoso estimates")
abline(0, 1)
```


\clearpage

```{r, echo=FALSE, eval = TRUE}
# this is forced to be evaluated!
stime <- "unknown"
if (identical(Sys.getenv("NOT_CRAN"), "true")) {
  stime <- round(as.numeric(difftime(time2 = compile_start,
                                     time1 = Sys.time(), 
                                     units = "mins")), 
                 digits = 1)
}
```

This document took `r stime` minutes to compile.

\clearpage

# References
